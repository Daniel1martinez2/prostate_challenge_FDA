{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prostate Cancer Worshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoadingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pq.read_table('data/df_train.parquet').to_pandas()\n",
    "df_test = pq.read_table('data/df_test.parquet').to_pandas()\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\n",
    "    'Cant_gr_flia', \n",
    "    'Cant_riesgos_flia_mean', \n",
    "    'cantidad_serv_flia', \n",
    "    'CANTIDAD_SERVICIOS', \n",
    "    'conteo_dx_diferentes', \n",
    "    'EDAD', \n",
    "    'psa_max_gr_flia', \n",
    "    'psa_min_gr_flia', \n",
    "    'Pendiente', \n",
    "    'Pendiente_flia', \n",
    "    'Promedio_costo', \n",
    "    'Promedio_costo_flia', \n",
    "    'psa_max_gr_flia', \n",
    "    'psa_min_gr_flia', \n",
    "    'MEDICAMENTOS', \n",
    "    'MEDICINA ESPECIALIZADA', \n",
    "    'MEDICINA GENERAL', \n",
    "    'TIEMPO_AFILIACION', \n",
    "    'TIEMPO_ULTIMA_CITA', \n",
    "    'PERDIDA_DE_PESO', \n",
    "    'Intercepto', \n",
    "    'Intercepto_flia', \n",
    "    'Target',\n",
    "    'Cant_Fliar_CP', \n",
    "    'Cant_Fliar_riesgos'\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    'AGRUPACION_DIASTOLICA', \n",
    "    'AGRUPACION_SISTOLICA', \n",
    "    'CANCER_MAMA_FAMILIAR', \n",
    "    'CANCER_OTRO_SITIO', \n",
    "    'CORONARIOS', \n",
    "    'CANCER_OTRO_SITIO_FAMILIAR',\n",
    "    'CORONARIOS_FAMILIAR', \n",
    "    'CEREBRAL', \n",
    "    'CEREBRAL_FAMILIAR', \n",
    "    'DIABETES', \n",
    "    'DIABETES_FAMILIAR', \n",
    "    'ENFERMEDAD_RENAL', \n",
    "    'ENFERMEDAD_RENAL_FAMILIAR', \n",
    "    'HIPERTENSION', \n",
    "    'HIPERTENSION_FAMILIAR', \n",
    "    'OTROS_ANTECEDENTES_VASCULARES', \n",
    "    'RIESGOS', \n",
    "    'ESTADO_CIVI', \n",
    "    'estrato', \n",
    "    'parentesco', \n",
    "    'PROGRAMA', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_columns = [\n",
    "    'AGRUPACION_DIASTOLICA',\n",
    "    'AGRUPACION_SISTOLICA',\n",
    "    'HIPERTENSION',\n",
    "    'HIPERTENSION_FAMILIAR',\n",
    "    'RIESGOS',\n",
    "    'estrato'\n",
    "]\n",
    "\n",
    "nominal_columns = [\n",
    "    'CANCER_MAMA_FAMILIAR',\n",
    "    'CANCER_OTRO_SITIO',\n",
    "    'CORONARIOS',\n",
    "    'CANCER_OTRO_SITIO_FAMILIAR',\n",
    "    'CORONARIOS_FAMILIAR',\n",
    "    'CEREBRAL',\n",
    "    'CEREBRAL_FAMILIAR',\n",
    "    'DIABETES',\n",
    "    'DIABETES_FAMILIAR',\n",
    "    'ENFERMEDAD_RENAL',\n",
    "    'ENFERMEDAD_RENAL_FAMILIAR',\n",
    "    'OTROS_ANTECEDENTES_VASCULARES',\n",
    "    'ESTADO_CIVI',\n",
    "    'parentesco',\n",
    "    'PROGRAMA'                  \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_train.copy()\n",
    "for column in ordinal_columns + nominal_columns + ['IMC']:\n",
    "    df_encoded[column] = df_encoded[column].astype('category')\n",
    "X = df_encoded.drop(columns=['Target'])\n",
    "y = df_encoded['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', enable_categorical=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='teal')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance using XGBoost')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small = X_train.sample(frac=0.2, random_state=42)\n",
    "y_train_small = y_train.loc[X_train_small.index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['Cant_Fliar_riesgos', 'Cant_Fliar_CP', 'min_Tiempo_CP_Fliar', 'psa_min_gr_flia', 'psa_max_gr_flia', 'CANCER_MAMA_FAMILIAR', 'Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate dropping features\n",
    "- In order to be sure whether we decide to drop or not the already identified features, we will run a preliminary model to test with and without the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', enable_categorical=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Model 1: With all features\n",
    "X_all_features = df_encoded.drop(columns=['Target'])\n",
    "y = df_encoded['Target']\n",
    "\n",
    "f1_all_features = train_and_evaluate(X_all_features, y)\n",
    "print(f\"F1 Score with all features: {f1_all_features}\")\n",
    "\n",
    "# Model 2: Dropping variables with zero importance\n",
    "X_reduced_features = df_encoded.drop(columns=['Target'] + features_to_drop)\n",
    "\n",
    "f1_reduced_features = train_and_evaluate(X_reduced_features, y)\n",
    "print(f\"F1 Score after dropping zero-importance features: {f1_reduced_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping additional features, including `'Cant_Fliar_riesgos'`, `'Cant_Fliar_CP'`, `'min_Tiempo_CP_Fliar'`, `'psa_min_gr_flia'`, `'psa_max_gr_flia'`, and `'CANCER_MAMA_FAMILIAR'`, the model's performance improved. The F1 score increased from **0.5207** (with all features) to **0.5384** (after removing these features), indicating that simplifying the model by excluding both zero-importance features and those with minimal predictive power can enhance the modelâ€™s performance. By reducing noise from less significant features, the model was able to generalize better and make more accurate predictions, showcasing the benefits of feature selection in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline_________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_numeric_columns = [col for col in numeric_columns if col not in features_to_drop]\n",
    "updated_ordinal_columns = [col for col in ordinal_columns if col not in features_to_drop]\n",
    "updated_nominal_columns = [col for col in nominal_columns if col not in features_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features(df):\n",
    "    \"\"\"Add feature engineered columns.\"\"\"\n",
    "    # Age Binning\n",
    "    df['AgeGroup'] = pd.cut(df['EDAD'], bins=[0, 30, 50, 70, 100], labels=['Young', 'Middle-aged', 'Senior', 'Elderly'])\n",
    "    \n",
    "    # Service Usage Grouping\n",
    "    df['ServiceUsageGroup'] = pd.cut(df['CANTIDAD_SERVICIOS'], bins=[0, 5, 15, 50], labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    # Health Risk Score\n",
    "    df['HealthRiskScore'] = df[['HIPERTENSION', 'CEREBRAL', 'DIABETES', 'ENFERMEDAD_RENAL']].sum(axis=1)\n",
    "    \n",
    "    # Family History Score\n",
    "    df['FamilyHistoryRisk'] = df[['CANCER_OTRO_SITIO_FAMILIAR', 'CORONARIOS_FAMILIAR', 'CEREBRAL_FAMILIAR', 'DIABETES_FAMILIAR']].sum(axis=1)\n",
    "    \n",
    "    # BMI and Age Interaction\n",
    "    df['BMI_Age_Interaction'] = df['IMC'] * df['EDAD']\n",
    "    \n",
    "    # Log transformations for skewed features\n",
    "    df['log_Pendiente'] = np.log1p(df['Pendiente'])\n",
    "    df['log_Intercepto'] = np.log1p(df['Intercepto'])\n",
    "    df['log_Promedio_costo'] = np.log1p(df['Promedio_costo'])\n",
    "    \n",
    "    # Recency of Medical Interactions\n",
    "    df['RecentInteraction'] = pd.cut(df['TIEMPO_ULTIMA_CITA'], bins=[0, 30, 90, 365], labels=['Very Recent', 'Recent', 'Old'])\n",
    "    \n",
    "    # Service Intensity\n",
    "    df['ServiceIntensity'] = df['CANTIDAD_SERVICIOS'] / df['TIEMPO_AFILIACION']\n",
    "    \n",
    "    # Family Service Ratio\n",
    "    df['FamilyServiceRatio'] = df['cantidad_serv_flia'] / (df['CANTIDAD_SERVICIOS'] + 1)\n",
    "    \n",
    "    # Family Risk Ratio\n",
    "    df['FamilyRiskRatio'] = df['Cant_Fliar_riesgos'] / df['Cant_gr_flia']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return add_new_features(X.copy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder, PolynomialFeatures\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('winsorizer', Winsorizer(capping_method='quantiles', tail='right', fold=0.05)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    # ('poly', PolynomialFeatures(degree=2, interaction_only=True)),\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, updated_numeric_columns),\n",
    "        ('ord', ordinal_transformer, updated_ordinal_columns),\n",
    "        ('nom', nominal_transformer, updated_nominal_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('feature_engineering', FeatureEngineeringTransformer()),\n",
    "    ('drop_columns', 'passthrough'),\n",
    "    ('preprocessor', preprocessor)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying preprocessor pipeline\n",
    "- Imputation and dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_train.drop(columns=features_to_drop)\n",
    "# y = df_train['Target']\n",
    "\n",
    "# pipeline.fit(X)\n",
    "# X_train_transformed = pipeline.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the pipeline output into a readable data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_columns = (\n",
    "#     updated_numeric_columns + \n",
    "#     updated_ordinal_columns + \n",
    "#     list(pipeline.named_steps['preprocessor'].transformers_[2][1]['onehot'].get_feature_names_out(updated_nominal_columns))\n",
    "# )\n",
    "\n",
    "# X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=transformed_columns)\n",
    "# X_train_transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Revisar el conteo de valores atipicos !!!!!!!!!!!!!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_iqr(df, numeric_columns):\n",
    "#     \"\"\"\n",
    "#     This function takes a dataframe and returns a dataframe that contains \n",
    "#     the Interquartile Range (IQR) for each numeric column in the dataframe.\n",
    "    \n",
    "#     Parameters:\n",
    "#     df (pd.DataFrame): Input dataframe\n",
    "    \n",
    "#     Returns:\n",
    "#     pd.DataFrame: Dataframe containing IQR values for each numeric column\n",
    "#     \"\"\"\n",
    "#     # Select numeric columns from the dataframe\n",
    "#     df_numeric_columns = df[numeric_columns]\n",
    "    \n",
    "#     # Calculate Q1 (25th percentile) and Q3 (75th percentile) for each numeric column\n",
    "#     Q1 = df_numeric_columns.quantile(0.25)\n",
    "#     Q3 = df_numeric_columns.quantile(0.75)\n",
    "    \n",
    "#     # Calculate the Interquartile Range (IQR)\n",
    "#     IQR = Q3 - Q1\n",
    "    \n",
    "#     # Create a dataframe to store the IQR values\n",
    "#     iqr_df = pd.DataFrame({\n",
    "#         'Column': IQR.index,\n",
    "#         'IQR': IQR.values\n",
    "#     }).sort_values(by='IQR', ascending=False)\n",
    "    \n",
    "#     return iqr_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iqr_result = calculate_iqr(df_train, numeric_columns)\n",
    "# iqr_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iqr_result_after = calculate_iqr(X_train_transformed_df, updated_numeric_columns)\n",
    "# print(iqr_result_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train_small\n",
    "# y_train = y_train_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Again but with accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Updated preprocessing pipeline\n",
    "# # Make sure you have the correct categorization of columns: numeric, ordinal, and nominal\n",
    "\n",
    "\n",
    "\n",
    "# # Numeric columns: Winsorizing and scaling\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('winsorizer', Winsorizer(capping_method='quantiles', tail='right', fold=0.05)),\n",
    "#     ('scaler', StandardScaler()),\n",
    "# ])\n",
    "\n",
    "# # Ordinal columns: Impute and encode\n",
    "# ordinal_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "# ])\n",
    "\n",
    "# # Nominal columns: Impute and one-hot encode\n",
    "# nominal_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Added dense output for compatibility\n",
    "# ])\n",
    "\n",
    "# # Preprocessor to handle different types of columns\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, updated_numeric_columns),  # Process numeric columns\n",
    "#         ('ord', ordinal_transformer, updated_ordinal_columns),  # Process ordinal columns\n",
    "#         ('nom', nominal_transformer, updated_nominal_columns)   # Process nominal columns\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Ensure that the preprocessing pipeline is used before PCA in your pipeline\n",
    "# n_components_pca = 10  # Adjust based on your dataset\n",
    "\n",
    "# # Define the SVM evaluation function using accuracy as the metric\n",
    "# def svm_evaluate(C, gamma, kernel_choice):\n",
    "#     kernel = 'linear' if kernel_choice < 0.5 else 'rbf'\n",
    "    \n",
    "#     # Create a complete pipeline: Preprocessing + PCA + SVM\n",
    "#     model_pipeline = Pipeline([\n",
    "#         ('preprocessor', preprocessor),  # Include the preprocessing pipeline\n",
    "#         ('pca', PCA(n_components=n_components_pca, random_state=42)),  # Add PCA after preprocessing\n",
    "#         ('svm', SVC(C=C, gamma=gamma, kernel=kernel, probability=True))  # SVM with hyperparameters\n",
    "#     ])\n",
    "    \n",
    "#     # Perform K-fold cross-validation and return mean accuracy score\n",
    "#     accuracy_scores = cross_val_score(model_pipeline, X_train, y_train, cv=3, scoring='accuracy', verbose=0)\n",
    "    \n",
    "#     return accuracy_scores.mean()\n",
    "\n",
    "# # Define the parameter bounds for Bayesian Optimization\n",
    "# pbounds = {\n",
    "#     'C': (0.1, 10),        # Regularization parameter\n",
    "#     'gamma': (0.5, 2),     # Kernel coefficient for 'rbf'\n",
    "#     'kernel_choice': (0, 1)  # 0 for 'linear', 1 for 'rbf'\n",
    "# }\n",
    "\n",
    "# # Set up the Bayesian optimizer\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=svm_evaluate,\n",
    "#     pbounds=pbounds,\n",
    "#     random_state=42,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# # Run the optimization without the progress bar\n",
    "# optimizer.maximize(init_points=5, n_iter=10)\n",
    "\n",
    "# # Output the best parameters\n",
    "# best_params = optimizer.max\n",
    "# print(\"Best parameters found:\", best_params)\n",
    "\n",
    "# # Train the final SVM model with the best parameters\n",
    "# C_opt = best_params['params']['C']\n",
    "# gamma_opt = best_params['params']['gamma']\n",
    "# kernel_opt = 'linear' if best_params['params']['kernel_choice'] < 0.5 else 'rbf'\n",
    "\n",
    "# # Final pipeline with best hyperparameters\n",
    "# best_svm_model = Pipeline([\n",
    "#     ('preprocessor', preprocessor),  # Include the preprocessing pipeline\n",
    "#     ('pca', PCA(n_components=n_components_pca, random_state=42)),  # Add PCA\n",
    "#     ('svm', SVC(C=C_opt, gamma=gamma_opt, kernel=kernel_opt, probability=True))  # Best SVM model\n",
    "# ])\n",
    "\n",
    "# # Train the best model using the entire training dataset\n",
    "# best_svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# y_pred_test_proba = best_svm_model.predict(X_test)\n",
    "\n",
    "# # Evaluate using accuracy score on the test set\n",
    "# test_accuracy = accuracy_score(y_test, y_pred_test_proba)\n",
    "\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_array\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, columns):\n",
    "#         self.columns = columns  # Can be column names for DataFrame or indices for NumPy arrays\n",
    "    \n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, X):\n",
    "#         X = X.copy()\n",
    "#         if isinstance(X, pd.DataFrame):\n",
    "#             # If X is a DataFrame, apply log transformation using column names\n",
    "#             for col in self.columns:\n",
    "#                 if col in X.columns:\n",
    "#                     X[col] = np.log1p(X[col])  # log1p handles log(0) cases by doing log(1 + x)\n",
    "#         elif isinstance(X, np.ndarray):\n",
    "#             # If X is a NumPy array, apply log transformation using column indices\n",
    "#             for col_idx in self.columns:\n",
    "#                 if isinstance(col_idx, int) and col_idx < X.shape[1]:\n",
    "#                     X[:, col_idx] = np.log1p(X[:, col_idx])  # log1p handles log(0) cases\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported data format. Expected DataFrame or NumPy array.\")\n",
    "#         return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HandleInfValues(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, replace_with=np.nan):\n",
    "#         self.replace_with = replace_with\n",
    "    \n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, X):\n",
    "#         # Convert to numpy array if not already\n",
    "#         X = check_array(X, force_all_finite='allow-nan', dtype=np.float64)\n",
    "#         # Replace infinity or too large values with np.nan or other specified values\n",
    "#         X[np.isinf(X)] = self.replace_with\n",
    "#         X[X > 1e10] = self.replace_with  # You can adjust this threshold\n",
    "#         return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the preprocessing pipeline components\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('winsorizer', Winsorizer(capping_method='quantiles', tail='right', fold=0.05)),\n",
    "#     ('scaler', StandardScaler()),\n",
    "# ])\n",
    "\n",
    "# ordinal_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "# ])\n",
    "\n",
    "# nominal_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Added dense output for compatibility\n",
    "# ])\n",
    "\n",
    "# # Preprocessor to handle different types of columns\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, updated_numeric_columns),  # Process numeric columns\n",
    "#         ('ord', ordinal_transformer, updated_ordinal_columns),  # Process ordinal columns\n",
    "#         ('nom', nominal_transformer, updated_nominal_columns)   # Process nominal columns\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Define the SVM evaluation function using accuracy as the metric and adding PCA components as a hyperparameter\n",
    "# def svm_evaluate(C, gamma, kernel_choice, pca_components):\n",
    "#     # Map kernel_choice to the actual kernel\n",
    "#     kernel_options = ['linear', 'rbf', 'sigmoid']\n",
    "#     kernel = kernel_options[int(kernel_choice)]\n",
    "    \n",
    "#     # Create a complete pipeline: Preprocessing + PCA + SVM\n",
    "#     model_pipeline = Pipeline([\n",
    "#         ('preprocessor', preprocessor),  # Include the preprocessing pipeline\n",
    "#         ('pca', PCA(n_components=int(pca_components), random_state=42)),  # Add PCA with the given number of components\n",
    "#         ('svm', SVC(C=C, gamma=gamma, kernel=kernel, probability=True))  # SVM with hyperparameters\n",
    "#     ])\n",
    "    \n",
    "#     # Perform K-fold cross-validation and return mean accuracy score\n",
    "#     accuracy_scores = cross_val_score(model_pipeline, X_train, y_train, cv=3, scoring='accuracy', verbose=0)\n",
    "    \n",
    "#     return accuracy_scores.mean()\n",
    "\n",
    "# # Define the parameter bounds for Bayesian Optimization, including more kernel options and PCA components\n",
    "# pbounds = {\n",
    "#     'C': (0.1, 10),        # Regularization parameter\n",
    "#     'gamma': (0.5, 2),     # Kernel coefficient for 'rbf'\n",
    "#     'kernel_choice': (0, 2),  # 0 for 'linear', 1 for 'rbf', 2 for 'sigmoid'\n",
    "#     'pca_components': (2, min(len(X_train.columns), 20))  # PCA components: between 2 and 20 or total features\n",
    "# }\n",
    "\n",
    "# # Set up the Bayesian optimizer\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=svm_evaluate,\n",
    "#     pbounds=pbounds,\n",
    "#     random_state=42,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# # Run the optimization without the progress bar\n",
    "# optimizer.maximize(init_points=5, n_iter=10)\n",
    "\n",
    "# # Output the best parameters\n",
    "# best_params = optimizer.max\n",
    "# print(\"Best parameters found:\", best_params)\n",
    "\n",
    "# # Train the final SVM model with the best parameters\n",
    "# C_opt = best_params['params']['C']\n",
    "# gamma_opt = best_params['params']['gamma']\n",
    "# kernel_opt = ['linear', 'rbf', 'poly', 'sigmoid'][int(best_params['params']['kernel_choice'])]\n",
    "# pca_opt = int(best_params['params']['pca_components'])\n",
    "# # Final pipeline with best hyperparameters\n",
    "# best_svm_model = Pipeline([\n",
    "#     ('preprocessor', preprocessor),  # Include the preprocessing pipeline\n",
    "#     ('pca', PCA(n_components=pca_opt, random_state=42)),  # Add PCA with optimal components\n",
    "#     ('svm', SVC(C=C_opt, gamma=gamma_opt, kernel=kernel_opt, probability=True))  # Best SVM model\n",
    "# ])\n",
    "\n",
    "# # Train the best model using the entire training dataset\n",
    "# best_svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# y_pred_test = best_svm_model.predict(X_test)\n",
    "\n",
    "# test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the preprocessing pipeline components\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('winsorizer', Winsorizer(capping_method='quantiles', tail='right', fold=0.05)),\n",
    "#     ('scaler', StandardScaler()),\n",
    "# ])\n",
    "\n",
    "# ordinal_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "# ])\n",
    "\n",
    "# nominal_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Added dense output for compatibility\n",
    "# ])\n",
    "\n",
    "# # Preprocessor to handle different types of columns\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, updated_numeric_columns),  # Process numeric columns\n",
    "#         ('ord', ordinal_transformer, updated_ordinal_columns),  # Process ordinal columns\n",
    "#         ('nom', nominal_transformer, updated_nominal_columns)   # Process nominal columns\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Define the SVM evaluation function using accuracy as the metric and adding PCA components as a hyperparameter\n",
    "# def svm_evaluate(C, gamma, kernel_choice, pca_components, apply_pca):\n",
    "#     # Map kernel_choice to the actual kernel\n",
    "#     kernel_options = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "#     kernel = kernel_options[int(kernel_choice)]\n",
    "    \n",
    "#     # Create a complete pipeline: Preprocessing + Optional PCA + SVM\n",
    "#     steps = [('preprocessor', preprocessor)]\n",
    "    \n",
    "#     if apply_pca > 0.5:  # Add PCA if apply_pca is \"true\"\n",
    "#         steps.append(('pca', PCA(n_components=int(pca_components), random_state=42)))\n",
    "    \n",
    "#     steps.append(('svm', SVC(C=C, gamma=gamma, kernel=kernel, probability=True)))  # SVM with hyperparameters\n",
    "    \n",
    "#     model_pipeline = Pipeline(steps)\n",
    "    \n",
    "#     # Perform K-fold cross-validation and return mean accuracy score\n",
    "#     accuracy_scores = cross_val_score(model_pipeline, X_train, y_train, cv=5, scoring='accuracy', verbose=0)\n",
    "    \n",
    "#     return accuracy_scores.mean()\n",
    "\n",
    "# # Define the parameter bounds for Bayesian Optimization, including more kernel options and PCA components\n",
    "# pbounds = {\n",
    "#     'C': (0.001, 50),       # Wider range for regularization parameter\n",
    "#     'gamma': (1e-4, 3),       # Broaden gamma range for better exploration\n",
    "#     'kernel_choice': (0, 3),  # 0 for 'linear', 1 for 'rbf', 2 for 'poly', 3 for 'sigmoid'\n",
    "#     'pca_components': (2, min(len(X_train.columns), 20)),  # PCA components: between 2 and 20 or total features\n",
    "#     'apply_pca': (0, 1)  # Binary choice to apply PCA or not\n",
    "# }\n",
    "\n",
    "# # Set up the Bayesian optimizer\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=svm_evaluate,\n",
    "#     pbounds=pbounds,\n",
    "#     random_state=42,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# # Run the optimization without the progress bar\n",
    "# optimizer.maximize(init_points=2, n_iter=3)\n",
    "\n",
    "# # Output the best parameters\n",
    "# best_params = optimizer.max\n",
    "# print(\"Best parameters found:\", best_params)\n",
    "\n",
    "# # Train the final SVM model with the best parameters\n",
    "# C_opt = best_params['params']['C']\n",
    "# gamma_opt = best_params['params']['gamma']\n",
    "# kernel_opt = ['linear', 'rbf', 'poly', 'sigmoid'][int(best_params['params']['kernel_choice'])]\n",
    "# pca_opt = int(best_params['params']['pca_components'])\n",
    "# apply_pca_opt = best_params['params']['apply_pca']\n",
    "\n",
    "# # Final pipeline with best hyperparameters\n",
    "# steps = [('preprocessor', preprocessor)]\n",
    "\n",
    "# if apply_pca_opt > 0.5:\n",
    "#     steps.append(('pca', PCA(n_components=pca_opt, random_state=42)))  # Add PCA if optimal\n",
    "\n",
    "# steps.append(('svm', SVC(C=C_opt, gamma=gamma_opt, kernel=kernel_opt, probability=True)))  # Best SVM model\n",
    "\n",
    "# best_svm_model = Pipeline(steps)\n",
    "\n",
    "# # Train the best model using the entire training dataset\n",
    "# best_svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# y_pred_test = best_svm_model.predict(X_test)\n",
    "\n",
    "# test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder, PowerTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.decomposition import PCA\n",
    "# import numpy as np\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.linear_model import RidgeClassifier\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# # Preprocessing pipeline remains the same\n",
    "# # Define your preprocessing steps\n",
    "# # Updated numeric transformer: Adding PowerTransformer after Winsorization\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('winsorizer', Winsorizer(capping_method='quantiles', tail='right', fold=0.05)),\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('power_transform', PowerTransformer(method='yeo-johnson'))  # Transforms to stabilize variance\n",
    "# ])\n",
    "\n",
    "# # Ordinal transformer with option for target encoding\n",
    "# ordinal_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "# ])\n",
    "\n",
    "# # Nominal transformer with OneHotEncoder (could use TargetEncoder if needed)\n",
    "# nominal_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Can replace with TargetEncoder if high-dimension\n",
    "# ])\n",
    "\n",
    "# # Combine transformations using ColumnTransformer\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, updated_numeric_columns),\n",
    "#         ('ord', ordinal_transformer, updated_ordinal_columns),\n",
    "#         ('nom', nominal_transformer, updated_nominal_columns)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Function to evaluate a model using cross-validation\n",
    "# def evaluate_model(model, X_train, y_train):\n",
    "#     model_pipeline = Pipeline([\n",
    "#         ('preprocessor', preprocessor),\n",
    "#         ('pca', PCA(n_components=5, random_state=42)),  # Optional PCA step\n",
    "#         ('classifier', model)  # Insert the classifier model here\n",
    "#     ])\n",
    "#     # Perform cross-validation and return accuracy\n",
    "#     scores = cross_val_score(model_pipeline, X_train, y_train, cv=15, scoring='accuracy')\n",
    "#     return scores.mean()\n",
    "\n",
    "# # Models to evaluate\n",
    "# models = {\n",
    "#     'Logistic Regression': LogisticRegression(),\n",
    "#     'Random Forest': RandomForestClassifier(),\n",
    "#     'XGBoost': XGBClassifier(eval_metric='logloss'),\n",
    "#     'SVM': SVC(),\n",
    "#     'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "#     'Decision Tree': DecisionTreeClassifier(),\n",
    "#     'Gradient Boosting': GradientBoostingClassifier(),\n",
    "#     'Ridge Classifier': RidgeClassifier(),\n",
    "#     'SGD Classifier': SGDClassifier()\n",
    "# }\n",
    "\n",
    "# # Iterate through the models, evaluate each one\n",
    "# best_score = 0\n",
    "# best_model = None\n",
    "# for model_name, model in models.items():\n",
    "#     score = evaluate_model(model, X_train, y_train)\n",
    "#     print(f\"{model_name} Accuracy: {score:.4f}\")\n",
    "#     if score > best_score:\n",
    "#         best_score = score\n",
    "#         best_model = model_name\n",
    "\n",
    "# # Output the best model\n",
    "# print(f\"\\nBest model: {best_model} with accuracy: {best_score:.4f}\")\n",
    "\n",
    "# # After finding the best model, train it on the entire dataset and evaluate on test set\n",
    "# best_model_pipeline = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('pca', PCA(n_components=10, random_state=42)),\n",
    "#     ('classifier', models[best_model])\n",
    "# ])\n",
    "\n",
    "# best_model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate on test set\n",
    "# y_pred_test = best_model_pipeline.predict(X_test)\n",
    "# test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# print(f\"Test Accuracy of {best_model}: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimize random fores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the Random Forest evaluation function using accuracy as the metric\n",
    "# def rf_evaluate(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features):\n",
    "#     # Create the Random Forest model with hyperparameters passed from Bayesian optimization\n",
    "#     model = RandomForestClassifier(\n",
    "#         n_estimators=int(n_estimators),  # Number of trees\n",
    "#         max_depth=int(max_depth),        # Maximum depth of the tree\n",
    "#         min_samples_split=int(min_samples_split),  # Minimum samples required to split\n",
    "#         min_samples_leaf=int(min_samples_leaf),    # Minimum samples required in a leaf\n",
    "#         max_features=max_features,        # Number of features to consider for the best split\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1  # Use all available cores\n",
    "#     )\n",
    "    \n",
    "#     # Create a pipeline with preprocessor and RandomForestClassifier\n",
    "#     model_pipeline = Pipeline([\n",
    "#         ('preprocessor', preprocessor),  # Include the preprocessing pipeline\n",
    "#         ('rf', model)  # Random Forest with hyperparameters\n",
    "#     ])\n",
    "    \n",
    "#     # Perform K-fold cross-validation and return mean accuracy score\n",
    "#     accuracy_scores = cross_val_score(model_pipeline, X_train, y_train, cv=5, scoring='accuracy', verbose=0)\n",
    "    \n",
    "#     return accuracy_scores.mean()\n",
    "\n",
    "# # Define the parameter bounds for Bayesian Optimization\n",
    "# pbounds = {\n",
    "#     'n_estimators': (50, 500),            # Number of trees\n",
    "#     'max_depth': (5, 50),                 # Maximum depth of the trees\n",
    "#     'min_samples_split': (2, 20),         # Minimum number of samples to split a node\n",
    "#     'min_samples_leaf': (1, 10),          # Minimum number of samples per leaf\n",
    "#     'max_features': (0.1, 1.0)            # Number of features to consider for the best split\n",
    "# }\n",
    "\n",
    "# # Set up the Bayesian optimizer\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=rf_evaluate,\n",
    "#     pbounds=pbounds,\n",
    "#     random_state=42,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# # Run the optimization\n",
    "# optimizer.maximize(init_points=10, n_iter=25)\n",
    "\n",
    "# # Output the best parameters\n",
    "# best_params = optimizer.max\n",
    "# print(\"Best parameters found:\", best_params)\n",
    "\n",
    "# # Step 3: Use the optimized parameters to create the best Random Forest model\n",
    "# best_rf_model = Pipeline([\n",
    "#     ('preprocessor', preprocessor),  # Include the preprocessing pipeline\n",
    "#     ('rf', RandomForestClassifier(\n",
    "#         n_estimators=int(best_params['params']['n_estimators']),\n",
    "#         max_depth=int(best_params['params']['max_depth']),\n",
    "#         min_samples_split=int(best_params['params']['min_samples_split']),\n",
    "#         min_samples_leaf=int(best_params['params']['min_samples_leaf']),\n",
    "#         max_features=best_params['params']['max_features'],\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "# # Step 4: Train the best model using the entire training dataset\n",
    "# best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Step 5: Evaluate the optimized Random Forest model on the test set\n",
    "# y_pred_test = best_rf_model.predict(X_test)\n",
    "\n",
    "# # Calculate the accuracy on the test set\n",
    "# test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# print(f\"Test Accuracy of the optimized Random Forest model: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using random forest to predict the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Step 1: Best model obtained from the previous test is Random Forest\n",
    "# # Use the pre-defined preprocessing pipeline (`preprocessor`)\n",
    "\n",
    "# # Step 2: Recreate the pipeline using Random Forest with optimal hyperparameters (from your earlier Bayesian Optimization if applicable)\n",
    "# best_rf_model = Pipeline([\n",
    "#     ('preprocessor', preprocessor),  # Include the preprocessing pipeline\n",
    "#     ('rf', RandomForestClassifier(n_estimators=100, random_state=42))  # Adjust hyperparameters if you performed optimization\n",
    "# ])\n",
    "\n",
    "# # Step 3: Fit the Random Forest model on the full training data\n",
    "# best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Step 4: Transform the test data using the same preprocessor\n",
    "# X_test_transformed = best_rf_model.named_steps['preprocessor'].transform(df_test)\n",
    "\n",
    "# # Step 5: Make predictions on the df_test data\n",
    "# y_test_predictions = best_rf_model.predict(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Best parameters obtained from Bayesian Optimization\n",
    "# best_params = {\n",
    "#     'max_depth': 37,\n",
    "#     'max_features': 0.1,\n",
    "#     'min_samples_leaf': 1,\n",
    "#     'min_samples_split': 2,\n",
    "#     'n_estimators': 288\n",
    "# }\n",
    "\n",
    "# # Step 1: Use the optimized parameters to create the Random Forest model\n",
    "# best_rf_model = Pipeline([\n",
    "#     ('preprocessor', preprocessor),  # Include the preprocessing pipeline\n",
    "#     ('rf', RandomForestClassifier(\n",
    "#         n_estimators=int(best_params['n_estimators']),\n",
    "#         max_depth=int(best_params['max_depth']),\n",
    "#         min_samples_split=int(best_params['min_samples_split']),\n",
    "#         min_samples_leaf=int(best_params['min_samples_leaf']),\n",
    "#         max_features=best_params['max_features'],\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "# # Step 2: Train the model on the training data\n",
    "# best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Step 3: Transform the df_test using the preprocessor\n",
    "# X_test_transformed = best_rf_model.named_steps['preprocessor'].transform(df_test)\n",
    "\n",
    "# # Step 4: Make predictions on df_test using the trained model\n",
    "# y_test_predictions = best_rf_model.predict(df_test)\n",
    "\n",
    "# # Step 5: Output the predictions\n",
    "# print(\"Predictions on the df_test set:\")\n",
    "# print(y_test_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = pd.DataFrame({\n",
    "#     'ID': df_test.index,\n",
    "# \t'Target': y_test_predictions\n",
    "# })\n",
    "# final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to csv\n",
    "# final_df.to_csv('data/preditions_8.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG-boost prooooo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define the preprocessing pipeline components\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('winsorizer', Winsorizer(capping_method='quantiles', tail='right', fold=0.05)),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Added dense output for compatibility\n",
    "])\n",
    "\n",
    "# Preprocessor to handle different types of columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, updated_numeric_columns),  # Process numeric columns\n",
    "        ('ord', ordinal_transformer, updated_ordinal_columns),  # Process ordinal columns\n",
    "        ('nom', nominal_transformer, updated_nominal_columns)   # Process nominal columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the XGBoost evaluation function using accuracy as the metric\n",
    "def xgb_evaluate(learning_rate, n_estimators, max_depth, reg_alpha, reg_lambda, subsample, colsample_bytree):\n",
    "    # Create a complete pipeline: Preprocessing + XGBoost\n",
    "    model_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),  # Include the preprocessing pipeline\n",
    "        ('xgb', XGBClassifier(learning_rate=learning_rate, \n",
    "                              n_estimators=int(n_estimators), \n",
    "                              max_depth=int(max_depth), \n",
    "                              reg_alpha=reg_alpha,  # L1 regularization\n",
    "                              reg_lambda=reg_lambda,  # L2 regularization\n",
    "                              subsample=subsample,  # Subsample ratio\n",
    "                              colsample_bytree=colsample_bytree,  # Feature subsample ratio\n",
    "                              random_state=42, \n",
    "                              use_label_encoder=False, \n",
    "                              eval_metric='logloss'))  # XGBoost with hyperparameters\n",
    "    ])\n",
    "    \n",
    "    # Perform K-fold cross-validation and return mean accuracy score\n",
    "    accuracy_scores = cross_val_score(model_pipeline, X_train, y_train, cv=5, scoring='accuracy', verbose=0)\n",
    "    \n",
    "    return accuracy_scores.mean()\n",
    "\n",
    "# Define the parameter bounds for Bayesian Optimization, excluding PCA components\n",
    "pbounds = {\n",
    "    'learning_rate': (0.01, 0.1),  # Learning rate values suitable for gradual training\n",
    "    'n_estimators': (100, 1000),\n",
    "    'max_depth': (6, 12),  # Reduced max depth to prevent overfitting\n",
    "    'reg_alpha': (0.01, 0.7),  # Regularization ranges based on previous observations\n",
    "    'reg_lambda': (0.01, 0.7),\n",
    "    'subsample': (0.6, 0.9),  # Subsample to prevent overfitting\n",
    "    'colsample_bytree': (0.6, 0.9)  # Feature subsample ratio\n",
    "}\n",
    "\n",
    "# Set up the Bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=xgb_evaluate,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Run the optimization without the progress bar\n",
    "optimizer.maximize(init_points=15, n_iter=15)\n",
    "\n",
    "# Output the best parameters\n",
    "best_params = optimizer.max\n",
    "print(\"Best parameters found:\", best_params)\n",
    "\n",
    "# Train the final XGBoost model with the best parameters\n",
    "learning_rate_opt = best_params['params']['learning_rate']\n",
    "n_estimators_opt = int(best_params['params']['n_estimators'])\n",
    "max_depth_opt = int(best_params['params']['max_depth'])\n",
    "reg_alpha_opt = best_params['params']['reg_alpha']\n",
    "reg_lambda_opt = best_params['params']['reg_lambda']\n",
    "subsample_opt = best_params['params']['subsample']\n",
    "colsample_bytree_opt = best_params['params']['colsample_bytree']\n",
    "\n",
    "# Final pipeline with best hyperparameters\n",
    "best_xgb_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Include the preprocessing pipeline\n",
    "    ('xgb', XGBClassifier(learning_rate=learning_rate_opt, \n",
    "                          n_estimators=n_estimators_opt, \n",
    "                          max_depth=max_depth_opt, \n",
    "                          reg_alpha=reg_alpha_opt,  # L1 regularization\n",
    "                          reg_lambda=reg_lambda_opt,  # L2 regularization\n",
    "                          subsample=subsample_opt,  # Subsample ratio\n",
    "                          colsample_bytree=colsample_bytree_opt,  # Feature subsample ratio\n",
    "                          random_state=42, \n",
    "                          use_label_encoder=False, \n",
    "                          eval_metric='logloss'))  # Best XGBoost model\n",
    "])\n",
    "\n",
    "# Train the best model using the entire training dataset\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_test = best_xgb_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Display feature importances\n",
    "xgb_model = best_xgb_model.named_steps['xgb']  # Get the XGBoost model from the pipeline\n",
    "importances = xgb_model.feature_importances_\n",
    "sorted_indices = np.argsort(importances)[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = optimizer.max\n",
    "# print(\"Best parameters found:\", best_params)\n",
    "\n",
    "# # Extract the best parameters\n",
    "# learning_rate_opt = best_params['params']['learning_rate']\n",
    "# n_estimators_opt = int(best_params['params']['n_estimators'])\n",
    "# max_depth_opt = int(best_params['params']['max_depth'])\n",
    "# pca_opt = int(best_params['params']['pca_components'])\n",
    "# reg_alpha_opt = best_params['params']['reg_alpha']\n",
    "# reg_lambda_opt = best_params['params']['reg_lambda']\n",
    "# subsample_opt = best_params['params']['subsample']\n",
    "# colsample_bytree_opt = best_params['params']['colsample_bytree']\n",
    "\n",
    "# # Final pipeline with best hyperparameters\n",
    "# best_xgb_model = Pipeline([\n",
    "#     ('preprocessor', preprocessor),  # Include the preprocessing pipeline\n",
    "#     ('pca', PCA(n_components=pca_opt, random_state=42)),  # Add PCA with optimal components\n",
    "#     ('xgb', XGBClassifier(learning_rate=learning_rate_opt, \n",
    "#                           n_estimators=n_estimators_opt, \n",
    "#                           max_depth=max_depth_opt, \n",
    "#                           reg_alpha=reg_alpha_opt,  # L1 regularization\n",
    "#                           reg_lambda=reg_lambda_opt,  # L2 regularization\n",
    "#                           subsample=subsample_opt,  # Subsample ratio\n",
    "#                           colsample_bytree=colsample_bytree_opt,  # Feature subsample ratio\n",
    "#                           random_state=42, \n",
    "#                           use_label_encoder=False, \n",
    "#                           eval_metric='logloss'))  # Best XGBoost model\n",
    "# ])\n",
    "\n",
    "# # Train the best model using the entire training dataset\n",
    "# best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# y_pred_test = best_xgb_model.predict(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # Use the best parameters obtained from Bayesian Optimization\n",
    "# learning_rate_opt = 0.01\n",
    "# max_depth_opt = int(20.0)\n",
    "# n_estimators_opt = int(round(348.558))\n",
    "# pca_components_opt = int(round(20.0))  # Round PCA components to the nearest integer\n",
    "\n",
    "# # Recreate the pipeline with the best hyperparameters\n",
    "# best_xgb_model = Pipeline([\n",
    "#     ('preprocessor', preprocessor),  # Include the preprocessing pipeline\n",
    "#     ('pca', PCA(n_components=pca_components_opt, random_state=42)),  # Add PCA with the optimal number of components\n",
    "#     ('xgb', XGBClassifier(learning_rate=learning_rate_opt, \n",
    "#                           n_estimators=n_estimators_opt, \n",
    "#                           max_depth=max_depth_opt, \n",
    "#                           random_state=42, use_label_encoder=False, eval_metric='logloss'))  # XGBoost with best parameters\n",
    "# ])\n",
    "\n",
    "# # Fit the model on the training data\n",
    "# best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# # Transform the test data using the preprocessor\n",
    "# X_test_transformed = best_xgb_model.named_steps['preprocessor'].transform(df_test)\n",
    "\n",
    "# y_test_predictions = best_xgb_model.predict(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = pd.DataFrame({\n",
    "#     'ID': df_test.index,\n",
    "# \t'Target': y_pred_test\n",
    "# })\n",
    "# final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to csv\n",
    "# final_df.to_csv('data/preditions_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the final model\n",
    "# best_svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the final model on the test set using ROC AUC\n",
    "# y_pred_proba = best_svm_model.predict_proba(X_test)[:, 1]  # Probability estimates for the positive class\n",
    "# test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "# print(f\"Final ROC AUC Score on the test set: {test_roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar diferentes kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
