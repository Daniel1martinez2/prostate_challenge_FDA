{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder, PowerTransformer\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pq.read_table('data/df_train.parquet').to_pandas()\n",
    "df_test = pq.read_table('data/df_test.parquet').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\n",
    "    'Cant_gr_flia', \n",
    "    'Cant_riesgos_flia_mean', \n",
    "    'cantidad_serv_flia', \n",
    "    'CANTIDAD_SERVICIOS', \n",
    "    'conteo_dx_diferentes', \n",
    "    'EDAD', \n",
    "    'psa_max_gr_flia', \n",
    "    'psa_min_gr_flia', \n",
    "    'Pendiente', \n",
    "    'Pendiente_flia', \n",
    "    'Promedio_costo', \n",
    "    'Promedio_costo_flia', \n",
    "    'psa_max_gr_flia', \n",
    "    'psa_min_gr_flia', \n",
    "    'MEDICAMENTOS', \n",
    "    'MEDICINA ESPECIALIZADA', \n",
    "    'MEDICINA GENERAL', \n",
    "    'TIEMPO_AFILIACION', \n",
    "    'TIEMPO_ULTIMA_CITA', \n",
    "    'PERDIDA_DE_PESO', \n",
    "    'Intercepto', \n",
    "    'Intercepto_flia', \n",
    "    'Cant_Fliar_CP', \n",
    "    'Cant_Fliar_riesgos'\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    'AGRUPACION_DIASTOLICA', \n",
    "    'AGRUPACION_SISTOLICA', \n",
    "    'CANCER_MAMA_FAMILIAR', \n",
    "    'CANCER_OTRO_SITIO', \n",
    "    'CORONARIOS', \n",
    "    'CANCER_OTRO_SITIO_FAMILIAR',\n",
    "    'CORONARIOS_FAMILIAR', \n",
    "    'CEREBRAL', \n",
    "    'CEREBRAL_FAMILIAR', \n",
    "    'DIABETES', \n",
    "    'DIABETES_FAMILIAR', \n",
    "    'ENFERMEDAD_RENAL', \n",
    "    'ENFERMEDAD_RENAL_FAMILIAR', \n",
    "    'HIPERTENSION', \n",
    "    'HIPERTENSION_FAMILIAR', \n",
    "    'OTROS_ANTECEDENTES_VASCULARES', \n",
    "    'RIESGOS', \n",
    "    'ESTADO_CIVI', \n",
    "    'estrato', \n",
    "    'parentesco', \n",
    "    'PROGRAMA', \n",
    "]\n",
    "\n",
    "nominal_columns = [\n",
    "    'ESTADO_CIVI', 'PROGRAMA', 'parentesco', 'CANCER_MAMA_FAMILIAR', 'CANCER_OTRO_SITIO',\n",
    "    'CANCER_OTRO_SITIO_FAMILIAR', 'HIPERTENSION', 'HIPERTENSION_FAMILIAR',\n",
    "    'DIABETES', 'DIABETES_FAMILIAR', 'CORONARIOS', 'CORONARIOS_FAMILIAR',\n",
    "    'CEREBRAL', 'CEREBRAL_FAMILIAR', 'ENFERMEDAD_RENAL', 'ENFERMEDAD_RENAL_FAMILIAR',\n",
    "    'OTROS_ANTECEDENTES_VASCULARES'\n",
    "]\n",
    "\n",
    "ordinal_columns = ['estrato', 'AGRUPACION_SISTOLICA', 'AGRUPACION_DIASTOLICA', 'IMC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=['Target'])\n",
    "y = df_train['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_columns),\n",
    "        ('ord', ordinal_transformer, ordinal_columns),\n",
    "        ('nom', nominal_transformer, nominal_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb', XGBClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "feature_names = xgb_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "xgb_model = xgb_pipeline.named_steps['xgb']\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "\n",
    "sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "top_20_idx = sorted_idx[:20]\n",
    "top_20_features = feature_names[top_20_idx]\n",
    "\n",
    "def get_original_columns(features, feature_names):\n",
    "    original_columns = []\n",
    "    for feature in features:\n",
    "        original_col = feature.split('__')[1]\n",
    "        if (original_col in feature_names):\n",
    "            original_columns.append(original_col)\n",
    "    return list(set(original_columns))\n",
    "\n",
    "\n",
    "selected_numeric_columns = get_original_columns(top_20_features, numeric_columns)\n",
    "selected_ordinal_columns = get_original_columns(top_20_features, ordinal_columns)\n",
    "selected_nominal_columns = get_original_columns(top_20_features, nominal_columns)\n",
    "\n",
    "\n",
    "reduced_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, selected_numeric_columns),\n",
    "        ('ord', ordinal_transformer, selected_ordinal_columns),\n",
    "        ('nom', nominal_transformer, selected_nominal_columns)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest optimization\n",
    "- Best parameters found: {'target': 0.82463421122639, 'params': {'max_depth': 25.0, 'max_features': 0.2, 'min_samples_leaf': 1.0, 'min_samples_split': 2.0, 'n_components_pca': 6.0, 'n_estimators': 562.6480841236934}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# # Define the Random Forest evaluation function using accuracy as the metric\n",
    "# def rf_evaluate(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, n_components_pca):\n",
    "#     # Create a complete pipeline: Preprocessing + RandomForest\n",
    "#     model_pipeline = Pipeline([\n",
    "#         # Include the preprocessing pipeline\n",
    "#         ('preprocessor', reduced_preprocessor),\n",
    "#         ('pca', PCA(n_components=int(n_components_pca))),  # Add PCA after preprocessing\n",
    "#         ('rf', RandomForestClassifier(n_estimators=int(n_estimators),\n",
    "#                                       max_depth=int(max_depth),\n",
    "#                                       min_samples_split=int(min_samples_split),\n",
    "#                                       min_samples_leaf=int(min_samples_leaf),\n",
    "#                                       max_features=max_features,\n",
    "#                                       random_state=42))  # Random Forest with hyperparameters\n",
    "#     ])\n",
    "\n",
    "#     stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     # accuracy_scores = cross_val_score(\n",
    "#     #     model_pipeline, X_train, y_train, cv=stratified_kfold, scoring='accuracy', n_jobs=-1)\n",
    "#     roc_auc_scores = cross_val_score(\n",
    "#         model_pipeline, X_train, y_train, cv=stratified_kfold, scoring='roc_auc', n_jobs=-1\n",
    "#     )\n",
    "\n",
    "#     return roc_auc_scores.mean()\n",
    "\n",
    "# pbounds = {\n",
    "#     'n_estimators': (400, 600),\n",
    "#     'max_depth': (25, 35),\n",
    "#     'min_samples_split': (2, 8),\n",
    "#     'min_samples_leaf': (1, 8),\n",
    "#     'max_features': (0.1, 0.2),\n",
    "#     'n_components_pca': (1, 5)\n",
    "# }\n",
    "\n",
    "# # Set up the Bayesian optimizer\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=rf_evaluate,\n",
    "#     pbounds=pbounds,\n",
    "#     random_state=42,\n",
    "#     verbose=2  # Verbose to see progress\n",
    "# )\n",
    "\n",
    "# # Run the optimization\n",
    "# # 10 random points first, then 32 iterations of optimization\n",
    "# optimizer.maximize(init_points=10, n_iter=30)\n",
    "\n",
    "# # Output the best parameters\n",
    "# best_params = optimizer.max\n",
    "# print(\"Best parameters found:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.decomposition import PCA\n",
    "# import pandas as pd\n",
    "\n",
    "# # Best parameters from Bayesian Optimization\n",
    "# best_params = {\n",
    "#     'max_depth': int(25.452039720948832),\n",
    "#     'max_features': 0.2,\n",
    "#     'min_samples_leaf': int(1.0),\n",
    "#     'min_samples_split': int(2.0),\n",
    "#     'n_components_pca': int(5.0),\n",
    "#     'n_estimators': int(552.5290750051523)\n",
    "# }\n",
    "\n",
    "# # Create the final model pipeline with the best parameters\n",
    "# model_pipeline = Pipeline([\n",
    "#     ('preprocessor', reduced_preprocessor),  # Use your preprocessor from before\n",
    "#     ('pca', PCA(n_components=best_params['n_components_pca'])),  # PCA with the best component number\n",
    "#     ('rf', RandomForestClassifier(\n",
    "#         n_estimators=best_params['n_estimators'],\n",
    "#         max_depth=best_params['max_depth'],\n",
    "#         min_samples_split=best_params['min_samples_split'],\n",
    "#         min_samples_leaf=best_params['min_samples_leaf'],\n",
    "#         max_features=best_params['max_features'],\n",
    "#         random_state=42\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "# # Fit the model on the training data\n",
    "# model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# predictions = model_pipeline.predict(df_test)\n",
    "\n",
    "# # Step 8: Create submission file\n",
    "# final_df = pd.DataFrame({\n",
    "#     'ID': df_test.index,  # Assuming df_test has the ID as index or column\n",
    "#     'Target': predictions\n",
    "# })\n",
    "\n",
    "# # Save the submission\n",
    "# submission_file = 'submission_rf.csv'\n",
    "# final_df.to_csv(submission_file, index=False)\n",
    "# print(f\"Submission file {submission_file} created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN optimization\n",
    "- so far: Best parameters found: {'target': 0.7156158552806597, 'params': {'n_components_pca': 5.0, 'n_neighbors': 50.0, 'p': 2.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# # Define the KNN evaluation function using accuracy as the metric\n",
    "# def knn_evaluate(n_neighbors, p, n_components_pca):\n",
    "#     # Create a complete pipeline: Preprocessing + KNN\n",
    "#     model_pipeline = Pipeline([\n",
    "#         ('preprocessor', reduced_preprocessor),\n",
    "#         ('pca', PCA(n_components=int(n_components_pca))),\n",
    "#         ('knn', KNeighborsClassifier(n_neighbors=int(n_neighbors), p=int(p), n_jobs=-1))\n",
    "#     ])\n",
    "\n",
    "#     stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     # Cross-validation\n",
    "#     accuracy_scores = cross_val_score(\n",
    "#         model_pipeline, X_train, y_train, cv=stratified_kfold, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "#     return accuracy_scores.mean()\n",
    "\n",
    "# # Define parameter bounds for Bayesian Optimization\n",
    "# pbounds = {\n",
    "#     'n_neighbors': (3, 50),  # KNN neighbors range\n",
    "#     'p': (1, 2),  # Distance metric (1: Manhattan, 2: Euclidean)\n",
    "#     'n_components_pca': (2, 5)  # PCA components range\n",
    "# }\n",
    "\n",
    "# # Set up the Bayesian optimizer\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=knn_evaluate,\n",
    "#     pbounds=pbounds,\n",
    "#     random_state=42,\n",
    "#     verbose=2  # Verbose to see progress\n",
    "# )\n",
    "\n",
    "# # Run the optimization\n",
    "# # 10 random points first, then 30 iterations of optimization\n",
    "# optimizer.maximize(init_points=10, n_iter=30)\n",
    "\n",
    "# # Output the best parameters\n",
    "# best_params = optimizer.max\n",
    "# print(\"Best parameters found:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from bayes_opt import BayesianOptimization\n",
    "\n",
    "# # Define the SVM evaluation function using ROC AUC as the metric\n",
    "# def svm_evaluate(C, gamma, kernel, n_components_pca):\n",
    "#     # Convert kernel index to a valid kernel string\n",
    "#     kernel_options = ['linear', 'rbf', 'poly']\n",
    "#     kernel = kernel_options[int(kernel)]\n",
    "\n",
    "#     # Create a complete pipeline: Preprocessing + PCA + SVM\n",
    "#     model_pipeline = Pipeline([\n",
    "#         ('preprocessor', reduced_preprocessor),\n",
    "#         ('pca', PCA(n_components=int(n_components_pca))),  # Add PCA after preprocessing\n",
    "#         ('svm', SVC(C=C, gamma=gamma, kernel=kernel, random_state=42, probability=True))\n",
    "#     ])\n",
    "\n",
    "#     stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     # Use ROC AUC as the evaluation metric\n",
    "#     roc_auc_scores = cross_val_score(\n",
    "#         model_pipeline, X_train, y_train, cv=stratified_kfold, scoring='roc_auc', n_jobs=-1\n",
    "#     )\n",
    "\n",
    "#     return roc_auc_scores.mean()\n",
    "\n",
    "# # Parameter bounds for SVM optimization\n",
    "# pbounds = {\n",
    "#     'C': (0.1, 10),                # Regularization parameter\n",
    "#     'gamma': (0.0001, 1),          # Kernel coefficient\n",
    "#     'kernel': (0, 2),              # Kernel type (0: linear, 1: rbf, 2: poly)\n",
    "#     'n_components_pca': (1, 5)     # PCA components\n",
    "# }\n",
    "\n",
    "# # Set up the Bayesian optimizer\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=svm_evaluate,\n",
    "#     pbounds=pbounds,\n",
    "#     random_state=42,\n",
    "#     verbose=2  # Verbose to see progress\n",
    "# )\n",
    "\n",
    "# # Run the optimization\n",
    "# # 10 random points first, then 30 iterations of optimization\n",
    "# optimizer.maximize(init_points=10, n_iter=30)\n",
    "\n",
    "# # Output the best parameters\n",
    "# best_params = optimizer.max\n",
    "# print(\"Best parameters found:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.decomposition import PCA\n",
    "# import pandas as pd\n",
    "\n",
    "# # Best parameters from Bayesian Optimization for SVM\n",
    "# best_svm_params = {\n",
    "#     'C': 9.69511928814146,\n",
    "#     'gamma': 0.804355672916069,\n",
    "#     'kernel': round(0.9545838701817976),  # Convert to nearest integer for kernel selection\n",
    "#     'n_components_pca': int(4.640990598805696)  # Convert to integer for PCA\n",
    "# }\n",
    "\n",
    "# # Map the kernel index to the actual kernel type\n",
    "# kernel_options = ['linear', 'rbf', 'poly']\n",
    "# kernel = kernel_options[best_svm_params['kernel']]\n",
    "\n",
    "# # Create the SVM model pipeline with the best parameters\n",
    "# model_pipeline = Pipeline([\n",
    "#     ('preprocessor', reduced_preprocessor),  # Use your preprocessor from before\n",
    "#     ('pca', PCA(n_components=best_svm_params['n_components_pca'])),  # PCA with the best component number\n",
    "#     ('svm', SVC(\n",
    "#         C=best_svm_params['C'],\n",
    "#         gamma=best_svm_params['gamma'],\n",
    "#         kernel=kernel,\n",
    "#         random_state=42,\n",
    "#         probability=True  # To enable probability estimates for ROC AUC\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "# # Fit the model on the training data\n",
    "# model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set (probabilities for ROC AUC)\n",
    "# predictions_proba = model_pipeline.predict_proba(df_test)[:, 1]  # Get the probability for the positive class\n",
    "\n",
    "# # Prepare the submission dataframe (replace 'Id' with the actual ID column from your test set)\n",
    "# # Step 8: Create submission file\n",
    "# final_df = pd.DataFrame({\n",
    "#     'ID': df_test.index,  # Assuming df_test has the ID as index or column\n",
    "#     'Target': predictions_proba\n",
    "# })\n",
    "\n",
    "# # Save the submission\n",
    "# submission_file = 'submission.csv'\n",
    "# final_df.to_csv(submission_file, index=False)\n",
    "# print(f\"Submission file {submission_file} created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG-Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 8: Create submission file\n",
    "# final_df = pd.DataFrame({\n",
    "#     'ID': df_test.index,  # Assuming df_test has the ID as index or column\n",
    "#     'Target': y_test_predictions\n",
    "# })\n",
    "\n",
    "# # Save the submission\n",
    "# submission_file = 'submission.csv'\n",
    "# final_df.to_csv(submission_file, index=False)\n",
    "# print(f\"Submission file {submission_file} created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
